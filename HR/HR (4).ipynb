{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"bUz6PJy280my","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659764996503,"user_tz":-360,"elapsed":15057,"user":{"displayName":"Jain Jannath","userId":"14566309891482213922"}},"outputId":"0b2428e0-88df-4b4b-e01f-b63137698444"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pafy in /usr/local/lib/python3.7/dist-packages (0.5.5)\n","Requirement already satisfied: youtube-dl in /usr/local/lib/python3.7/dist-packages (2021.12.17)\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (0.2.3.5)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.4.2)\n","Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (2.4.1)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.64.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy) (1.21.6)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy) (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.7/dist-packages (0.4.7)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: imageio==2.4.1 in /usr/local/lib/python3.7/dist-packages (2.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.1) (1.21.6)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.1) (7.1.2)\n"]}],"source":["!pip install pafy youtube-dl moviepy\n","!pip install imageio-ffmpeg\n","!pip3 install imageio==2.4.1"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"60z-yVmc9DSz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659765007343,"user_tz":-360,"elapsed":6813,"user":{"displayName":"Jain Jannath","userId":"14566309891482213922"}},"outputId":"7693348b-bdf6-4deb-d974-81aa5692b58b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n","Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n","Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2703360/45929032 bytes (5.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6225920/45929032 bytes (13.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9412608/45929032 bytes (20.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12845056/45929032 bytes (28.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16531456/45929032 bytes (36.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20037632/45929032 bytes (43.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23355392/45929032 bytes (50.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26820608/45929032 bytes (58.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29720576/45929032 bytes (64.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32071680/45929032 bytes (69.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34512896/45929032 bytes (75.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36478976/45929032 bytes (79.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38395904/45929032 bytes (83.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41000960/45929032 bytes (89.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42565632/45929032 bytes (92.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44777472/45929032 bytes (97.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n","  Done\n","File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"]}],"source":["# Import the required libraries.\n","import os\n","import cv2\n","import pafy\n","import math\n","import random\n","import numpy as np\n","import datetime as dt\n","import tensorflow as tf\n","from collections import deque\n","import matplotlib.pyplot as plt\n"," \n","from moviepy.editor import *\n","%matplotlib inline\n"," \n","from sklearn.model_selection import train_test_split\n"," \n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.utils import plot_model"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ZipROYd09DPN","executionInfo":{"status":"ok","timestamp":1659765163482,"user_tz":-360,"elapsed":431,"user":{"displayName":"Jain Jannath","userId":"14566309891482213922"}}},"outputs":[],"source":["\n","seed_constant = 27\n","np.random.seed(seed_constant)\n","random.seed(seed_constant)\n","tf.random.set_seed(seed_constant)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wztDRsiT9DMs"},"outputs":[],"source":["# Discard the output of this cell.\n","%%capture\n"," \n","# Downlaod the UCF50 Dataset\n","!wget --no-check-certificate https://www.crcv.ucf.edu/data/UCF50.rar\n"," \n","#Extract the Dataset\n","!unrar x UCF50.rar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWAX8F079DKA"},"outputs":[],"source":["plt.figure(figsize = (20, 20))\n"," \n","# Get the names of all classes/categories in UCF50.\n","all_classes_names = os.listdir('UCF50')\n"," \n","# Generate a list of 20 random values. The values will be between 0-50, \n","# where 50 is the total number of class in the dataset. \n","random_range = random.sample(range(len(all_classes_names)), 20)\n"," \n","# Iterating through all the generated random values.\n","for counter, random_index in enumerate(random_range, 1):\n"," \n","    # Retrieve a Class Name using the Random Index.\n","    selected_class_Name = all_classes_names[random_index]\n"," \n","    # Retrieve the list of all the video files present in the randomly selected Class Directory.\n","    video_files_names_list = os.listdir(f'UCF50/{selected_class_Name}')\n"," \n","    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.\n","    selected_video_file_name = random.choice(video_files_names_list)\n"," \n","    # Initialize a VideoCapture object to read from the video File.\n","    video_reader = cv2.VideoCapture(f'UCF50/{selected_class_Name}/{selected_video_file_name}')\n","    \n","    # Read the first frame of the video file.\n","    _, bgr_frame = video_reader.read()\n"," \n","    # Release the VideoCapture object. \n","    video_reader.release()\n"," \n","    # Convert the frame from BGR into RGB format. \n","    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n"," \n","    # Write the class name on the video frame.\n","    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n","    \n","    # Display the frame.\n","    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ousN0wBX9DHO"},"outputs":[],"source":["# Specify the height and width to which each video frame will be resized in our dataset.\n","IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64\n"," \n","# Specify the number of frames of a video that will be fed to the model as one sequence.\n","SEQUENCE_LENGTH = 20\n"," \n","# Specify the directory containing the UCF50 dataset. \n","DATASET_DIR = \"UCF50\"\n"," \n","# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.\n","CLASSES_LIST = [\"WalkingWithDog\", \"TaiChi\", \"Swing\", \"HorseRace\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYsIjyze9DEg"},"outputs":[],"source":["def frames_extraction(video_path):\n","    '''\n","    This function will extract the required frames from a video after resizing and normalizing them.\n","    Args:\n","        video_path: The path of the video in the disk, whose frames are to be extracted.\n","    Returns:\n","        frames_list: A list containing the resized and normalized frames of the video.\n","    '''\n"," \n","    # Declare a list to store video frames.\n","    frames_list = []\n","    \n","    # Read the Video File using the VideoCapture object.\n","    video_reader = cv2.VideoCapture(video_path)\n"," \n","    # Get the total number of frames in the video.\n","    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n"," \n","    # Calculate the the interval after which frames will be added to the list.\n","    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n"," \n","    # Iterate through the Video Frames.\n","    for frame_counter in range(SEQUENCE_LENGTH):\n"," \n","        # Set the current frame position of the video.\n","        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n"," \n","        # Reading the frame from the video. \n","        success, frame = video_reader.read() \n"," \n","        # Check if Video frame is not successfully read then break the loop\n","        if not success:\n","            break\n"," \n","        # Resize the Frame to fixed height and width.\n","        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n","        \n","        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n","        normalized_frame = resized_frame / 255\n","        \n","        # Append the normalized frame into the frames list\n","        frames_list.append(normalized_frame)\n","    \n","    # Release the VideoCapture object. \n","    video_reader.release()\n"," \n","    # Return the frames list.\n","    return frames_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HaTjO6xP9DBr"},"outputs":[],"source":["def create_dataset():\n","    '''\n","    This function will extract the data of the selected classes and create the required dataset.\n","    Returns:\n","        features:          A list containing the extracted frames of the videos.\n","        labels:            A list containing the indexes of the classes associated with the videos.\n","        video_files_paths: A list containing the paths of the videos in the disk.\n","    '''\n"," \n","    # Declared Empty Lists to store the features, labels and video file path values.\n","    features = []\n","    labels = []\n","    video_files_paths = []\n","    \n","    # Iterating through all the classes mentioned in the classes list\n","    for class_index, class_name in enumerate(CLASSES_LIST):\n","        \n","        # Display the name of the class whose data is being extracted.\n","        print(f'Extracting Data of Class: {class_name}')\n","        \n","        # Get the list of video files present in the specific class name directory.\n","        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n","        \n","        # Iterate through all the files present in the files list.\n","        for file_name in files_list:\n","            \n","            # Get the complete video path.\n","            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n"," \n","            # Extract the frames of the video file.\n","            frames = frames_extraction(video_file_path)\n"," \n","            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.\n","            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n","            if len(frames) == SEQUENCE_LENGTH:\n"," \n","                # Append the data to their repective lists.\n","                features.append(frames)\n","                labels.append(class_index)\n","                video_files_paths.append(video_file_path)\n"," \n","    # Converting the list to numpy arrays\n","    features = np.asarray(features)\n","    labels = np.array(labels)  \n","    \n","    # Return the frames, class index, and video file path.\n","    return features, labels, video_files_paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Upb1pIQ9C-0"},"outputs":[],"source":["# Create the dataset.\n","features, labels, video_files_paths = create_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cpK-0_JO9C73"},"outputs":[],"source":["\n","\n","1\n","2\n","# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors\n","one_hot_encoded_labels = to_categorical(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pN0Zbv9R9C5c"},"outputs":[],"source":["def create_convlstm_model():\n","    '''\n","    This function will construct the required convlstm model.\n","    Returns:\n","        model: It is the required constructed convlstm model.\n","    '''\n"," \n","    # We will use a Sequential model for model construction\n","    model = Sequential()\n"," \n","    # Define the Model Architecture.\n","    ########################################################################################################################\n","    \n","    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = \"channels_last\",\n","                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,\n","                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n","    \n","    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n","    model.add(TimeDistributed(Dropout(0.2)))\n","    \n","    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n","                         recurrent_dropout=0.2, return_sequences=True))\n","    \n","    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n","    model.add(TimeDistributed(Dropout(0.2)))\n","    \n","    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n","                         recurrent_dropout=0.2, return_sequences=True))\n","    \n","    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n","    model.add(TimeDistributed(Dropout(0.2)))\n","    \n","    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n","                         recurrent_dropout=0.2, return_sequences=True))\n","    \n","    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n","    #model.add(TimeDistributed(Dropout(0.2)))\n","    \n","    model.add(Flatten()) \n","    \n","    model.add(Dense(len(CLASSES_LIST), activation = \"softmax\"))\n","    \n","    ########################################################################################################################\n","     \n","    # Display the models summary.\n","    model.summary()\n","    \n","    # Return the constructed convlstm model.\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iz2WpIDY9C2z"},"outputs":[],"source":["# Construct the required convlstm model.\n","convlstm_model = create_convlstm_model()\n"," \n","# Display the success message. \n","print(\"Model Created Successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inLcVxnV9Czz"},"outputs":[],"source":["# Plot the structure of the contructed model.\n","plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hIDXg6mfA6nJ"},"outputs":[],"source":["\n","1\n","2\n","# Split the Data into Train ( 75% ) and Test Set ( 25% ).\n","features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.25, shuffle = True, random_state = seed_constant)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"84yXdoaF9CxD","outputId":"4b8fd29f-8295-46a1-dd01-e4bee4f66de3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","73/73 [==============================] - 154s 2s/step - loss: 1.3930 - accuracy: 0.2466 - val_loss: 1.3814 - val_accuracy: 0.4110\n","Epoch 2/50\n","73/73 [==============================] - 151s 2s/step - loss: 1.3489 - accuracy: 0.3801 - val_loss: 1.3114 - val_accuracy: 0.4795\n","Epoch 3/50\n","73/73 [==============================] - 147s 2s/step - loss: 1.1817 - accuracy: 0.5068 - val_loss: 1.1934 - val_accuracy: 0.4247\n","Epoch 4/50\n","73/73 [==============================] - 146s 2s/step - loss: 0.9222 - accuracy: 0.6301 - val_loss: 1.0649 - val_accuracy: 0.5205\n","Epoch 5/50\n","39/73 [===============>..............] - ETA: 1:03 - loss: 0.6990 - accuracy: 0.7244"]}],"source":["\n","# Create an Instance of Early Stopping Callback\n","early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)\n"," \n","# Compile the model and specify loss function, optimizer and metrics values to the model\n","convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n"," \n","# Start training the model.\n","convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 50, batch_size = 4,shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nb24lqaq9CuZ"},"outputs":[],"source":["\n","\n","# Evaluate the trained model.\n","model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDOjWUNS9Crp"},"outputs":[],"source":["\n","# Get the loss and accuracy from model_evaluation_history.\n","model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n"," \n","# Define the string date format.\n","# Get the current Date and Time in a DateTime Object.\n","# Convert the DateTime object to string according to the style mentioned in date_time_format string.\n","date_time_format = '%Y_%m_%d__%H_%M_%S'\n","current_date_time_dt = dt.datetime.now()\n","current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n"," \n","# Define a useful name for our model to make it easy for us while navigating through multiple saved models.\n","model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n"," \n","# Save your Model.\n","convlstm_model.save(model_file_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-aOdHYmX9Coj"},"outputs":[],"source":["\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n","    '''\n","    This function will plot the metrics passed to it in a graph.\n","    Args:\n","        model_training_history: A history object containing a record of training and validation \n","                                loss values and metrics values at successive epochs\n","        metric_name_1:          The name of the first metric that needs to be plotted in the graph.\n","        metric_name_2:          The name of the second metric that needs to be plotted in the graph.\n","        plot_name:              The title of the graph.\n","    '''\n","    \n","    # Get metric values using metric names as identifiers.\n","    metric_value_1 = model_training_history.history[metric_name_1]\n","    metric_value_2 = model_training_history.history[metric_name_2]\n","    \n","    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.\n","    epochs = range(len(metric_value_1))\n"," \n","    # Plot the Graph.\n","    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n","    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)\n"," \n","    # Add title to the plot.\n","    plt.title(str(plot_name))\n"," \n","    # Add legend to the plot.\n","    plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpmMq2-A9CmA"},"outputs":[],"source":["\n","\n","# Visualize the training and validation loss metrices.\n","plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1g1ao-s9CjZ"},"outputs":[],"source":["\n","\n","# Visualize the training and validation accuracy metrices.\n","plot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy') "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nd6S4Ba19Cgx"},"outputs":[],"source":["\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","def create_LRCN_model():\n","    '''\n","    This function will construct the required LRCN model.\n","    Returns:\n","        model: It is the required constructed LRCN model.\n","    '''\n"," \n","    # We will use a Sequential model for model construction.\n","    model = Sequential()\n","    \n","    # Define the Model Architecture.\n","    ########################################################################################################################\n","    \n","    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),\n","                              input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n","    \n","    model.add(TimeDistributed(MaxPooling2D((4, 4)))) \n","    model.add(TimeDistributed(Dropout(0.25)))\n","    \n","    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))\n","    model.add(TimeDistributed(MaxPooling2D((4, 4))))\n","    model.add(TimeDistributed(Dropout(0.25)))\n","    \n","    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n","    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","    model.add(TimeDistributed(Dropout(0.25)))\n","    \n","    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n","    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","    #model.add(TimeDistributed(Dropout(0.25)))\n","                                      \n","    model.add(TimeDistributed(Flatten()))\n","                                      \n","    model.add(LSTM(32))\n","                                      \n","    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n"," \n","    ########################################################################################################################\n"," \n","    # Display the models summary.\n","    model.summary()\n","    \n","    # Return the constructed LRCN model.\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EzLMHhAC9CeT"},"outputs":[],"source":["\n","# Construct the required LRCN model.\n","LRCN_model = create_LRCN_model()\n"," \n","# Display the success message.\n","print(\"Model Created Successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMQmKF6K9Cb0"},"outputs":[],"source":["\n","1\n","2\n","# Plot the structure of the contructed LRCN model.\n","plot_model(LRCN_model, to_file = 'LRCN_model_structure_plot.png', show_shapes = True, show_layer_names = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0DSSwttU9CZG"},"outputs":[],"source":["\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","# Create an Instance of Early Stopping Callback.\n","early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)\n"," \n","# Compile the model and specify loss function, optimizer and metrics to the model.\n","LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n"," \n","# Start training the model.\n","LRCN_model_training_history = LRCN_model.fit(x = features_train, y = labels_train, epochs = 50, batch_size = 4 , shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83kWvkm49CWY"},"outputs":[],"source":["\n","1\n","2\n","# Evaluate the trained model.\n","model_evaluation_history = LRCN_model.evaluate(features_test, labels_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hYuaQLm9CTu"},"outputs":[],"source":["\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","# Get the loss and accuracy from model_evaluation_history.\n","model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n"," \n","# Define the string date format.\n","# Get the current Date and Time in a DateTime Object.\n","# Convert the DateTime object to string according to the style mentioned in date_time_format string.\n","date_time_format = '%Y_%m_%d__%H_%M_%S'\n","current_date_time_dt = dt.datetime.now()\n","current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n","    \n","# Define a useful name for our model to make it easy for us while navigating through multiple saved models.\n","model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n"," \n","# Save the Model.\n","LRCN_model.save(model_file_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6v_9-CSM9CRH"},"outputs":[],"source":["\n","1\n","2\n","# Visualize the training and validation loss metrices.\n","plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l0PBtweI9COZ"},"outputs":[],"source":["\n","1\n","2\n","# Visualize the training and validation accuracy metrices.\n","plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WxZKBK7u9CLz"},"outputs":[],"source":["\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","def download_youtube_videos(youtube_video_url, output_directory):\n","     '''\n","    This function downloads the youtube video whose URL is passed to it as an argument.\n","    Args:\n","        youtube_video_url: URL of the video that is required to be downloaded.\n","        output_directory:  The directory path to which the video needs to be stored after downloading.\n","    Returns:\n","        title: The title of the downloaded youtube video.\n","    '''\n"," \n","     # Create a video object which contains useful information about the video.\n","     video = pafy.new(youtube_video_url)\n"," \n","     # Retrieve the title of the video.\n","     title = video.title\n"," \n","     # Get the best available quality object for the video.\n","     video_best = video.getbest()\n"," \n","     # Construct the output file path.\n","     output_file_path = f'{output_directory}/{title}.mp4'\n"," \n","     # Download the youtube video at the best available quality and store it to the contructed path.\n","     video_best.download(filepath = output_file_path, quiet = True)\n"," \n","     # Return the video title.\n","     return title"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"faJpZ33KIxXT"},"outputs":[],"source":["!pip install git+https://github.com/Cupcakus/pafy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TsM9E3IAMK_s"},"outputs":[],"source":["!pip uninstall -y pafy\n","!pip install git+https://github.com/Cupcakus/pafy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TgLm1v-q9CJS"},"outputs":[],"source":["\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","# Make the Output directory if it does not exist\n","test_videos_directory = 'test_videos'\n","os.makedirs(test_videos_directory, exist_ok = True)\n","\n"," \n","# Download a YouTube Video.\n","video_title = download_youtube_videos('https://www.youtube.com/watch?v=8u0qjmHIOcE', test_videos_directory)\n"," \n","# Get the YouTube Video's path we just downloaded.\n","input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LL5vkgN09CG3"},"outputs":[],"source":["\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):\n","    '''\n","    This function will perform action recognition on a video using the LRCN model.\n","    Args:\n","    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.\n","    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.\n","    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.\n","    '''\n"," \n","    # Initialize the VideoCapture object to read from the video file.\n","    video_reader = cv2.VideoCapture(video_file_path)\n"," \n","    # Get the width and height of the video.\n","    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n"," \n","    # Initialize the VideoWriter Object to store the output video in the disk.\n","    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), \n","                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n"," \n","    # Declare a queue to store video frames.\n","    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n"," \n","    # Initialize a variable to store the predicted action being performed in the video.\n","    predicted_class_name = ''\n"," \n","    # Iterate until the video is accessed successfully.\n","    while video_reader.isOpened():\n"," \n","        # Read the frame.\n","        ok, frame = video_reader.read() \n","        \n","        # Check if frame is not read properly then break the loop.\n","        if not ok:\n","            break\n"," \n","        # Resize the Frame to fixed Dimensions.\n","        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n","        \n","        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.\n","        normalized_frame = resized_frame / 255\n"," \n","        # Appending the pre-processed frame into the frames list.\n","        frames_queue.append(normalized_frame)\n"," \n","        # Check if the number of frames in the queue are equal to the fixed sequence length.\n","        if len(frames_queue) == SEQUENCE_LENGTH:\n"," \n","            # Pass the normalized frames to the model and get the predicted probabilities.\n","            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]\n"," \n","            # Get the index of class with highest probability.\n","            predicted_label = np.argmax(predicted_labels_probabilities)\n"," \n","            # Get the class name using the retrieved index.\n","            predicted_class_name = CLASSES_LIST[predicted_label]\n"," \n","        # Write predicted class name on top of the frame.\n","        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n"," \n","        # Write The frame into the disk using the VideoWriter Object.\n","        video_writer.write(frame)\n","        \n","    # Release the VideoCapture and VideoWriter objects.\n","    video_reader.release()\n","    video_writer.release()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1MQ70D2uZng8wVj7_QO9dKmRnAdnmwJNN"},"id":"giaVioFT9CEP","outputId":"2bac4cfd-357f-4646-99ab-ced4ef4c1bd2"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","# Construct the output video path.\n","output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'\n"," \n","# Perform Action Recognition on the Test Video.\n","predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)\n"," \n","# Display the output video.\n","VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X5LMi6359CB4"},"outputs":[],"source":["\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","def predict_single_action(video_file_path, SEQUENCE_LENGTH):\n","    '''\n","    This function will perform single action recognition prediction on a video using the LRCN model.\n","    Args:\n","    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.\n","    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.\n","    '''\n"," \n","    # Initialize the VideoCapture object to read from the video file.\n","    video_reader = cv2.VideoCapture(video_file_path)\n"," \n","    # Get the width and height of the video.\n","    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n"," \n","    # Declare a list to store video frames we will extract.\n","    frames_list = []\n","    \n","    # Initialize a variable to store the predicted action being performed in the video.\n","    predicted_class_name = ''\n"," \n","    # Get the number of frames in the video.\n","    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n"," \n","    # Calculate the interval after which frames will be added to the list.\n","    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH),1)\n"," \n","    # Iterating the number of times equal to the fixed length of sequence.\n","    for frame_counter in range(SEQUENCE_LENGTH):\n"," \n","        # Set the current frame position of the video.\n","        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n"," \n","        # Read a frame.\n","        success, frame = video_reader.read() \n"," \n","        # Check if frame is not read properly then break the loop.\n","        if not success:\n","            break\n"," \n","        # Resize the Frame to fixed Dimensions.\n","        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n","        \n","        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.\n","        normalized_frame = resized_frame / 255\n","        \n","        # Appending the pre-processed frame into the frames list\n","        frames_list.append(normalized_frame)\n"," \n","    # Passing the  pre-processed frames to the model and get the predicted probabilities.\n","    predicted_labels_probabilities = convlstm_model.predict(np.expand_dims(frames_list, axis = 0))[0]\n"," \n","    # Get the index of class with highest probability.\n","    predicted_label = np.argmax(predicted_labels_probabilities)\n"," \n","    # Get the class name using the retrieved index.\n","    predicted_class_name = CLASSES_LIST[predicted_label]\n","    \n","    # Display the predicted action along with the prediction confidence.\n","    print(f'Action Predicted: {predicted_class_name}\\nConfidence: {predicted_labels_probabilities[predicted_label]}')\n","        \n","    # Release the VideoCapture object. \n","    video_reader.release()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1Bn6JkZLQcBEPxDq36UAEgArHBQxk1GeB"},"id":"7ad6qMwU9B_F","outputId":"4986cb99-6f8c-4258-9b4c-c3117fbba5d1"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","# Download the youtube video.\n","video_title = download_youtube_videos('https://www.youtube.com/watch?v=XqqpZS0c1K0', test_videos_directory)\n"," \n","# Construct tihe nput youtube video path\n","input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'\n"," \n","# Perform Single Prediction on the Test Video.\n","predict_single_action(input_video_file_path, SEQUENCE_LENGTH)\n"," \n","# Display the input video.\n","VideoFileClip(input_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73B_nGxh9B8o"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ui7P0RwX9B6D"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TaSvLSCn9B3a"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZrqHr8f9BzD"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HRortYke9Bt_"},"outputs":[],"source":[""]}],"metadata":{"colab":{"name":"Untitled3 (4).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}